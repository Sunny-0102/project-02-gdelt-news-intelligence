# Operations Runbook — Project 02: GDELT News Intelligence

This file is the “how to run and keep it working” guide for the repo: refresh pipeline, validate outputs, publish to BigQuery for Tableau, and fix common breakages.

---

## 1) What this project does

**Goal:** Build a reproducible pipeline that turns raw GDELT data into:
- **Clean daily event aggregates** (by country + event category)
- **Daily country “risk” signals** derived from event composition + tone
- **Next-day risk forecasts** per country
- **Tableau dashboards** (Topic Pulse + Risk Monitor) reading from BigQuery

**Source:** Public GDELT BigQuery dataset  
- `gdelt-bq.gdeltv2.events_partitioned`

**Destination (your BigQuery dataset):**
- `gen-lang-client-0366281238.gdelt_portfolio`

---

## 2) Repo structure (what matters for ops)

- `src/` pipeline scripts
- `data/extracts/` raw extracts generated by the pipeline (not tracked in git)
- `data/processed/` cleaned outputs (not tracked in git)
- `reports/` markdown logs + saved figures/screenshots (tracked in git)
- `tableau/` Tableau workbook assets + exported images (tracked in git)

**BigQuery tables created by this project**
- `gdelt_portfolio.events_daily_clean`
- `gdelt_portfolio.country_risk_daily`
- `gdelt_portfolio.country_risk_forecasts_next_day`
- `gdelt_portfolio.gkg_theme_daily_YYYYMMDD_YYYYMMDD` (if using the GKG theme V2 table)

---

## 3) One-time setup (only do once per machine)

### 3.1 Python environment
From repo root:
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

### 3.2 Google auth (Application Default Credentials)

- gcloud auth login
- gcloud auth application-default login
- gcloud config set project gen-lang-client-0366281238
- gcloud auth application-default set-quota-project gen-lang-client-0366281238

### 3.3 Create BigQuery dataset (only once)

python - << 'PY'
from google.cloud import bigquery
client = bigquery.Client(project="gen-lang-client-0366281238")
dataset_id = f"{client.project}.gdelt_portfolio"
dataset = bigquery.Dataset(dataset_id)
dataset.location = "US"
client.create_dataset(dataset, exists_ok=True)
print("Dataset ready:", dataset_id)
PY

---

## 4) Daily refresh (end-to-end)
### Run these from repo root (with .venv active):

- python src/extract_events_daily.py
- python src/clean_events_daily.py
- python src/publish_tableau_table.py
- python src/create_country_risk_daily_table.py
- python src/publish_risk_forecasts.py

### What each step does

- extract_events_daily.py
Pulls daily aggregates from gdelt-bq.gdeltv2.events_partitioned into data/extracts/events_daily_*.csv.
- clean_events_daily.py
Standardizes types, adds labels/buckets, writes data/processed/events_daily_clean.parquet, and writes a QA report to reports/data_quality_events_daily.md.
- publish_tableau_table.py
Pushes the clean dataset into BigQuery as gdelt_portfolio.events_daily_clean for Tableau.
- create_country_risk_daily_table.py
Builds gdelt_portfolio.country_risk_daily (daily features + derived risk score).
- publish_risk_forecasts.py
Trains a next-day model per country and publishes a “latest snapshot” table to gdelt_portfolio.country_risk_forecasts_next_day.

---

## 5) Validation checks (do after every refresh)

### 5.1 Confirm extract is current
python - << 'PY'
import pandas as pd
df = pd.read_csv("data/extracts/events_daily_20251001_20260111.csv")
print("extract max(SQLDATE):", df["SQLDATE"].max())
print("extract min(SQLDATE):", df["SQLDATE"].min())
PY

### 5.2 Confirm BigQuery clean table is current
python - << 'PY'
from google.cloud import bigquery
c = bigquery.Client(project="gen-lang-client-0366281238")

q1 = "SELECT MAX(date) AS max_date FROM `gen-lang-client-0366281238.gdelt_portfolio.events_daily_clean`"
q2 = "SELECT MAX(date) AS max_date FROM `gen-lang-client-0366281238.gdelt_portfolio.country_risk_daily`"
q3 = "SELECT MAX(forecast_date) AS max_forecast_date FROM `gen-lang-client-0366281238.gdelt_portfolio.country_risk_forecasts_next_day`"

print("events_daily_clean max(date):", list(c.query(q1).result())[0]["max_date"])
print("country_risk_daily max(date):", list(c.query(q2).result())[0]["max_date"])
print("country_risk_forecasts_next_day max(forecast_date):", list(c.query(q3).result())[0]["max_forecast_date"])
PY

### Expected logic
- events_daily_clean max(date) should match the most recent day you extracted/cleaned.
- country_risk_daily max(date) should match events_daily_clean max(date).
- country_risk_forecasts_next_day max(forecast_date) should be country_risk_daily max(date) + 1 day.

---

## 6) Tableau operations (Risk Monitor + Forecast)

### 6.1 Data sources in Tableau (BigQuery → dataset → tables)

Use dataset: gdelt_portfolio
Add these tables to the data model:

- country_risk_daily
- country_risk_forecasts_next_day

### 6.2 Relationship (critical)

You already applied this, keep it documented here:
- CountryCode (daily) = Country Code (forecast)
- Date (daily) = As Of Date (forecast)
This makes the forecast “dot” sit on a real day (the last day we actually observed), while the dot value is the next-day prediction.

### 6.3 Filters you should standardize in Tableau

- Country Code: single-select (default US)
- Date: range (entire history) BUT KPI calcs should use “Latest Date in Range”
- Latest Run Date (from forecast table): keep only the latest run
    - If you don’t filter this, Tableau may show multiple runs and visuals can break or duplicate.

---

## 7) Tableau troubleshooting playbook

### Problem A: country_risk_forecasts_next_day doesn’t show in Tableau
1. Confirm it exists in BigQuery:

python - << 'PY'
from google.cloud import bigquery
c = bigquery.Client(project="gen-lang-client-0366281238")
q = """
SELECT COUNT(*) AS row_count
FROM `gen-lang-client-0366281238.gdelt_portfolio.country_risk_forecasts_next_day`
"""
print(list(c.query(q).result())[0]["row_count"])
PY


2. In Tableau Data Source page:
    - Click the refresh icon near the connection
    - Re open the dataset dropdown → gdelt_portfolio
    - Use search in the table list: type forecast


### Problem B: Forecast dates stop at 12/31/2025
This means Tableau is reading a table snapshot that only contains forecasts up to that date.

Fix sequence:
1. Confirm the real upstream source is newer:
    - Extract max(SQLDATE) from the CSV extract
    - Confirm GDELT itself has newer (events_partitioned max(SQLDATE))

2. If extract is newer but BigQuery tables are stuck:
    - Re-run (in order):
    clean_events_daily.py → publish_tableau_table.py → create_country_risk_daily_table.py → publish_risk_forecasts.py

3. Re-check:
    - events_daily_clean max(date)
    - country_risk_daily max(date)
    - country_risk_forecasts_next_day max(forecast_date)


### Problem C: KPI sheets show blank (no number)

Common causes and fixes:
- The KPI measure is not on the Text mark
    - Marks card → ensure the KPI field is dropped onto Text
- Date filter excludes latest day
    - Temporarily widen the Date filter range
- You’re using the wrong aggregation
    - KPI should usually be MAX([Metric (Latest)]) (not SUM)
- Relationship mismatch
    - Re-check relationship keys (Country + Date = As Of Date)
- Forecast run-date filter removes everything
    - Temporarily remove the “Latest Run Date” filter to confirm data exists, then re-add it correctly


### Problem D: “Risk Trend + Forecast” becomes blank when added to dashboard
This happens when the worksheet relies on fields from both tables and dashboard-level filters break the context.

Fix sequence:
1. Open the worksheet (not the dashboard) and confirm it renders there first.
2. In the dashboard:
    - Apply the Date filter only to worksheets that use country_risk_daily
    - Apply the Latest Run Date filter only to worksheets that use the forecast table

3. If it still blanks:
    - Remove the dashboard filters one-by-one until it returns
    - Re-add them with Apply to Selected Worksheets… instead of “All using this data source”


### Problem E: You see “two charts” in Risk Trend + Forecast
That’s almost always because you have multiple date pills (e.g., DAY(Date) and MONTH(Date)) or duplicated axes.

Fix:
- Keep one date field on Columns: Date as Exact Date (Continuous)
- Use Dual Axis only for:
    - Actual risk line (from daily)
    - Forecast dot (from forecast)
- Sync axis and hide duplicate headers.

---

## 8) Evidence + documentation required for GitHub (what to commit)

### 8.1 Always commit
- README.md (overview + architecture + how to run)
- reports/data_quality_events_daily.md
- reports/submissions/ or reports/figures/ images

### 8.2 Tableau screenshots
Save and commit:
- reports/figures/risk_monitor.png
- reports/figures/risk_monitor_2.png (your updated version)

### 8.3 Tableau workbook assets
Store Tableau files under:
- tableau/
    - gdelt_global_pulse.twb (or .twbx if you want packaged)

---

## 9) Release discipline (V1 / V2 / V3 tags)

Use tags so recruiters can see milestones:
- v1-topic-pulse
- v2-theme-pulse
- v3-risk-monitor-forecast

Template:

git status
git add -A
git commit -m "V3: Risk Monitor + next-day risk forecasts"
git tag v3-risk-monitor-forecast
git push
git push --tags

---

## 10) Common dependency errors

Missing optional dependency 'tabulate'
pip install tabulate

1. Pandas dtype warnings on CSV reads

Not fatal, but if you want to silence it:
- Use low_memory=False or specify dtype= in the reading script.

---

## 11) Safety rules (do not break these)

- Never commit .venv/
- Never commit data/ raw extracts or processed datasets
- Never commit any credential files under ~/.config/gcloud/
- Commit reports + screenshots + code only


:contentReference[oaicite:0]{index=0}
::contentReference[oaicite:1]{index=1}

---

```bash
